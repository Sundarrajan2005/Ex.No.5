

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

## DATE: 26.9.2025
## REG NO: 212223060279

# Aim: 
To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required:

* ChatGPT (AI Language Model)

* Any other text-based AI tool (optional, for cross-verification)

# Explanation: 

## Types of Prompting Patterns
1. Naïve Prompt (Broad/Unstructured):

* General instructions with no structure or constraints.

* Often lacks context, word limit, or focus.

Example: “Write about the internet.”

2.Basic Prompt (Clear, Structured, Refined):

* Detailed, clear, and well-framed instructions.

* Provides context, format requirements, and goals.

Example: “Write a 100-word note on the internet, including its definition, two advantages, and one disadvantage.”

## Reason for Experiment:

AI models are highly sensitive to prompt quality. By testing naïve vs. basic prompts, we can observe how structured instructions improve AI performance in terms of:

* Quality of Output

* Accuracy of Information

* Depth of Explanation

* Relevance to User’s Needs

## Test Scenarios

We considered 5 scenarios for comprehensive evaluation:

* Creative Story Generation

* Answering a Factual Question

* Summarization of a Concept

* Providing Advice / Recommendation

* Comparative / Analytical Task

## OUTPUT:

<img width="1182" height="641" alt="image" src="https://github.com/user-attachments/assets/36123b25-c056-4baf-a737-498f6173b997" />


## Evaluation Criteria:


We evaluate results on three parameters:

1. Quality – How well the response matches user intent.

2. Naïve prompts: Often generic, incomplete.

3. Basic prompts: Detailed, structured, user-focused.




# RESULT:
The experiment was executed successfully. It is concluded that clear, detailed, and structured prompts consistently generate outputs of higher quality, accuracy, and depth compared to naïve prompts.
